{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8889d4c2-d7e5-4360-a0cd-45a6c808159b",
   "metadata": {},
   "source": [
    "## __Fine-Tuning Versi 1__\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e1dbd9-7962-45af-a399-def539f17363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggunakan device: cuda\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: IMPORT STANDARD & ATUR DEVICE\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Sklearn untuk metrik\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, classification_report\n",
    "\n",
    "# Atur device (periksa GPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Menggunakan device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e25eb4-deca-4904-9a53-0215eb2d9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: IMPLEMENTASI zeroPadding_3D\n",
    "\n",
    "def zeroPadding_3D(old_matrix, pad_length, pad_depth=0):\n",
    "    \"\"\"\n",
    "    old_matrix: numpy array (H, W, B)\n",
    "    pad_length: jumlah pad di spatial (keempat arah)\n",
    "    pad_depth: optional, (default 0)\n",
    "    \"\"\"\n",
    "    new_matrix = np.pad(old_matrix, ((pad_length, pad_length), (pad_length, pad_length), (pad_depth, pad_depth)),\n",
    "                        mode='constant', constant_values=0)\n",
    "    return new_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cb2ba74-2050-4e9c-bfc6-7b0b4148fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: DEFINISI MODEL (encoder, transformer, projection head)\n",
    "\n",
    "class SpectralSpatialEncoder3D(nn.Module):\n",
    "    def __init__(self, embedding_dim=256, init_channels=32):\n",
    "        super().__init__()\n",
    "        # Konvolusi pertama (non-overlapping subpatch)\n",
    "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=init_channels,\n",
    "                               kernel_size=(20,3,3), stride=(20,3,3), padding=0)\n",
    "        self.bn1 = nn.BatchNorm3d(init_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        # Konvolusi kedua: linear projection ke embedding_dim\n",
    "        self.conv2 = nn.Conv3d(in_channels=init_channels, out_channels=embedding_dim,\n",
    "                               kernel_size=(1,1,1), stride=(1,1,1), padding=0)\n",
    "        self.bn2 = nn.BatchNorm3d(embedding_dim)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,1,224,9,9)\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))   # -> (B, init_ch, 11, 3, 3)\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))   # -> (B, 256, 11, 3, 3)\n",
    "        B, C, D, H, W = x.shape\n",
    "        # Permute dan flatten token axis -> (B, 99, 256)\n",
    "        x = x.permute(0,2,3,4,1).contiguous().view(x.size(0), -1, x.size(1))\n",
    "        return x\n",
    "\n",
    "class SimpleTransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_heads=8, num_layers=5, mlp_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
    "                                           dim_feedforward=mlp_dim, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,99,256) -> keluar (B,99,256)\n",
    "        return self.transformer(x)\n",
    "\n",
    "class ProjectionHead_A(nn.Module): # Projection Head VERSI A\n",
    "    def __init__(self, in_dim=256, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(in_dim, proj_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,99,256)\n",
    "        x = x.mean(dim=1)   # Global average pooling antar token -> (B,256)\n",
    "        return self.net(x)  # -> (B,128)\n",
    "    \n",
    "class ProjectionHead_B(nn.Module): # Projection Head VERSI B\n",
    "    def __init__(self, in_dim=256, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(in_dim, proj_dim)\n",
    "\n",
    "    def forward(self, x):      # x: (B, 99, 256)\n",
    "        x = self.net(x)        #  (B, 99, 128)  # proyeksi per-token\n",
    "        x = x.mean(dim=1)      #  (B, 128)      # pooling global antar token\n",
    "        return x\n",
    "\n",
    "class ProjectionHead_C(nn.Module): # Projection Head VERSI C\n",
    "    def __init__(self, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(99, proj_dim)  # 99 ke 128\n",
    "\n",
    "    def forward(self, x):  # x: (B, 99, 256)\n",
    "        x = x.mean(dim=2)        # (B, 99, 1)  # GAP Dalam Token\n",
    "        x = x.squeeze(-1)        # (B, 99)\n",
    "        return self.net(x)        # (B, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d8088b6-975a-4642-93ce-f4c5b42566fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: LOAD HASIL PRETRAINING dan FREEZE semua kecuali classifier nanti\n",
    "\n",
    "def build_frozen_parts_from_best_pretrained_model(variant, device):\n",
    "    \"\"\"\n",
    "    Mengembalikan tiga komponen yang sudah dimuat:\n",
    "    encoder, transformer, proj_head (semua parameter dibekukan)\n",
    "    \"\"\"\n",
    "    # instantiate model parts\n",
    "    encoder = SpectralSpatialEncoder3D(embedding_dim=256).to(device)\n",
    "    transformer = SimpleTransformerEncoder(embed_dim=256).to(device)\n",
    "\n",
    "    if variant == 'A':\n",
    "        proj_head = ProjectionHead_A(in_dim=256, proj_dim=128).to(device)\n",
    "        best_model_path = f\"best_sst_ver3{variant}.pt\" # ini file pretrained best model ver3A\n",
    "    elif variant == 'B' :\n",
    "        proj_head = ProjectionHead_B(in_dim=256, proj_dim=128).to(device)\n",
    "        best_model_path = f\"best_sst_ver3{variant}.pt\" # ini file pretrained best model ver3B\n",
    "    elif variant == 'C':\n",
    "        proj_head = ProjectionHead_C(proj_dim=128).to(device)\n",
    "        best_model_path = f\"best_sst_ver3{variant}.pt\" # ini file pretrained best model ver3C\n",
    "    else:\n",
    "        raise ValueError(\"variant must be 'A'/'B'/'C'\")\n",
    "\n",
    "    # load best model yang sudah dilatih sebelumnya\n",
    "    assert os.path.exists(best_model_path), f\"Best model tidak ditemukan: {best_model_path}\"\n",
    "    bm_point = torch.load(best_model_path, map_location=device) #bm_point untuk menampung best model yang di-load\n",
    "\n",
    "    # muat state dict (jaga kompatibilitas)\n",
    "    if \"encoder_state\" in bm_point:\n",
    "        encoder.load_state_dict(bm_point[\"encoder_state\"])\n",
    "    if \"transformer_state\" in bm_point:\n",
    "        transformer.load_state_dict(bm_point[\"transformer_state\"])\n",
    "    if \"proj_head_state\" in bm_point:\n",
    "        try:\n",
    "            proj_head.load_state_dict(bm_point[\"proj_head_state\"])\n",
    "        except Exception as e:\n",
    "            # coba non-strict load bila ada mismatch minor\n",
    "            print(\"[PERINGATAN] proj_head.load_state_dict error -> mencoba strict=False. Error:\", e)\n",
    "            proj_head.load_state_dict(bm_point[\"proj_head_state\"], strict=False)\n",
    "\n",
    "    # Freeze param agar tidak ikut update saat fine-tuning\n",
    "    for p in encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in transformer.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in proj_head.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Memastikan modul-modul beku dalam mode eval agar BatchNorm/Dropout stabil\n",
    "    encoder.eval()\n",
    "    transformer.eval()\n",
    "    proj_head.eval()\n",
    "\n",
    "    return encoder, transformer, proj_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea4bc443-29d8-4c0e-bb64-d5c854125d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: FTClassifier (hanya classifier yang trainable)\n",
    "\n",
    "class FTClassifier(nn.Module):\n",
    "    def __init__(self, encoder, transformer, proj_head, num_classes=2):\n",
    "        super().__init__()\n",
    "        # komponen beku (sudah di-freeze sebelumnya)\n",
    "        self.encoder = encoder\n",
    "        self.transformer = transformer\n",
    "        self.proj_head = proj_head\n",
    "        # classifier linear sederhana\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Semua feature extraction dilakukan tanpa grad untuk menghemat memori\n",
    "        with torch.no_grad():\n",
    "            feat = self.encoder(x)        # (B,99,256)\n",
    "            feat = self.transformer(feat) # (B,99,256)\n",
    "            proj = self.proj_head(feat)   # (B,128)\n",
    "        logits = self.classifier(proj)    # (B,2)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079df931-97d9-4359-ba9a-6558e3eef266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: HELPERS - memuat dataset patch\n",
    "\n",
    "def load_patch_dataset(data_dir=\"../data/processed\", batch_size=32, val_split=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Memuat patch_class0.npy dan patch_class1.npy,\n",
    "    menggabungkan, kemudian membagi ke train/val\n",
    "    Output: train_loader, val_loader\n",
    "    \"\"\"\n",
    "    path0 = os.path.join(data_dir, \"patch_class0.npy\")\n",
    "    path1 = os.path.join(data_dir, \"patch_class1.npy\")\n",
    "    assert os.path.exists(path0) and os.path.exists(path1), \"File patch_class*.npy tidak ditemukan\"\n",
    "\n",
    "    p0 = np.load(path0)  # shape (N0, 9,9,224)\n",
    "    p1 = np.load(path1)  # shape (N1, 9,9,224)\n",
    "    X_all = np.concatenate([p0, p1], axis=0)\n",
    "    y_all = np.concatenate([np.zeros(len(p0)), np.ones(len(p1))], axis=0)\n",
    "\n",
    "    # ubah ke tensor PyTorch format conv3d: (N,1,224,9,9)\n",
    "    X_tensor = torch.tensor(X_all, dtype=torch.float32).unsqueeze(1).permute(0,1,4,2,3)\n",
    "    y_tensor = torch.tensor(y_all, dtype=torch.long)\n",
    "\n",
    "    # split train/val konsisten\n",
    "    N = len(X_tensor)\n",
    "    rng = torch.Generator().manual_seed(seed)\n",
    "    indices = torch.randperm(N, generator=rng)\n",
    "    val_size = int(val_split * N)\n",
    "    val_idx = indices[:val_size]\n",
    "    train_idx = indices[val_size:]\n",
    "\n",
    "    train_ds = TensorDataset(X_tensor[train_idx], y_tensor[train_idx])\n",
    "    val_ds = TensorDataset(X_tensor[val_idx], y_tensor[val_idx])\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "502d0cf6-cf08-4c6b-b24b-9049f9ef0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: TRAINING LOOP untuk fine-tuning classifier (hanya classifier param yang dioptimasi)\n",
    "\n",
    "def train_finetune(model, variant, train_loader, val_loader, device,\n",
    "                   num_epochs=200, lr=1e-3, weight_decay=1e-4, patience=50):\n",
    "    optimizer = optim.AdamW(model.classifier.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve = 0\n",
    "    start_epoch = 1\n",
    "    \n",
    "    \n",
    "    checkpoint_finetuned_path = f\"checkpoint_sst_finetuned_ver3{variant}.pt\" # ini file finetuned checkpoint untuk versi {variant}\n",
    "    best_finetuned_path = f\"best_finetuned_ver3{variant}.pt\" # ini file finetuned best model untuk versi {variant}\n",
    "    \n",
    "    # ==== Jika checkpoint ada, lanjutkan dari sana ====\n",
    "    if os.path.exists(checkpoint_finetuned_path):\n",
    "        checkpoint = torch.load(checkpoint_finetuned_path, map_location=device)\n",
    "        model.classifier.load_state_dict(checkpoint[\"classifier_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "        print(f\"[OK] Checkpoint ditemukan. Melanjutkan dari epoch {start_epoch}.\")\n",
    "    else:\n",
    "        print(\"[MAAF] Tidak ditemukan checkpoint. Memulai training dari awal.\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs+1):\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} - Train\")\n",
    "        for x_batch, y_batch in pbar:\n",
    "            # DEBUG (hapus setelah selesai cek)\n",
    "            # if start_epoch == epoch and n == 0:\n",
    "            #     print(\"DEBUG Batch shapes -> x_batch:\", x_batch.shape, \" y_batch:\", y_batch.shape)\n",
    "                \n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(x_batch)\n",
    "\n",
    "            loss = criterion(logits, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * x_batch.size(0)\n",
    "            n += x_batch.size(0)\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_train_loss = total_loss / n\n",
    "\n",
    "        # validasi\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        nval = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for xv, yv in val_loader:\n",
    "                xv = xv.to(device); yv = yv.to(device)\n",
    "                logits = model(xv)\n",
    "                lossv = criterion(logits, yv)\n",
    "                val_loss += lossv.item() * xv.size(0)\n",
    "                nval += xv.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == yv).sum().item()\n",
    "        avg_val_loss = val_loss / nval\n",
    "        val_acc = correct / nval\n",
    "\n",
    "        # Waktu per epoch\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}]\" \n",
    "              f\"TrainLoss: {avg_train_loss:.4f} |\"\n",
    "              f\"ValLoss: {avg_val_loss:.4f} |\" \n",
    "              f\"ValAcc: {val_acc:.4f}|\"\n",
    "              f\"Time: {epoch_time:.2f}s\")\n",
    "\n",
    "        checkpoint = {\n",
    "            \"epoch\" : epoch,\n",
    "            \"classifier_state\" : model.classifier.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"best_val_loss\" : best_val_loss\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_finetuned_path)\n",
    "\n",
    "        # checkpoint best classifier\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            no_improve = 0\n",
    "            torch.save(checkpoint, best_finetuned_path)\n",
    "            print(\">> Model fine-tuned terbaik disimpan:\", best_finetuned_path)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(\"Selesai training fine-tuning.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6306b505-85b2-4dbf-96e9-ff1090b55776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah sampel train: 8 | jumlah batch train: 1\n",
      "Jumlah sampel val: 2 | jumlah batch val: 1\n"
     ]
    }
   ],
   "source": [
    "# CELL 8: EKSEKUSI DATA LOADER\n",
    "\n",
    "# (dari cell 6 : Data Loader)\n",
    "# Contoh muat data\n",
    "train_loader, val_loader = load_patch_dataset(data_dir=\"../data/processed\", batch_size=32, val_split=0.2)\n",
    "print(\"Jumlah sampel train:\", sum(len(batch[0]) for batch in train_loader), \"| jumlah batch train:\", len(train_loader))\n",
    "print(\"Jumlah sampel val:\", sum(len(batch[0]) for batch in val_loader), \"| jumlah batch val:\", len(val_loader))\n",
    "\n",
    "# kosongkan cache dan cek memori gpu\n",
    "if device.startswith('cuda'):\n",
    "    torch.cuda.empty_cache()\n",
    "    # !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a40127ff-45ea-41ed-85f3-7e1f6eda42ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_14020\\1638809886.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bm_point = torch.load(best_model_path, map_location=device) #bm_point untuk menampung best model yang di-load\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Komponen pra-trained telah dimuat dan dibekukan.\n",
      "Jumlah parameter yang dilatih (harus hanya classifier): 258\n",
      "[MAAF] Tidak ditemukan checkpoint. Memulai training dari awal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 - Train: 100%|█| 1/1 [00:00<00:00,  4.39it/s, loss=0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200]TrainLoss: 0.7203 |ValLoss: 0.9997 |ValAcc: 0.0000|Time: 0.25s\n",
      ">> Model fine-tuned terbaik disimpan: best_finetuned_ver3A.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 - Train: 100%|█| 1/1 [00:00<00:00, 100.33it/s, loss=0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/200]TrainLoss: 0.7043 |ValLoss: 1.0213 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200 - Train: 100%|█| 1/1 [00:00<00:00, 132.25it/s, loss=0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/200]TrainLoss: 0.6862 |ValLoss: 1.0427 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/200 - Train: 100%|█| 1/1 [00:00<00:00, 182.11it/s, loss=0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/200]TrainLoss: 0.6618 |ValLoss: 1.0652 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 - Train: 100%|█| 1/1 [00:00<00:00, 133.31it/s, loss=0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/200]TrainLoss: 0.6421 |ValLoss: 1.0885 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/200 - Train: 100%|█| 1/1 [00:00<00:00, 333.49it/s, loss=0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/200]TrainLoss: 0.6271 |ValLoss: 1.1120 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/200 - Train: 100%|█| 1/1 [00:00<00:00, 136.46it/s, loss=0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/200]TrainLoss: 0.6121 |ValLoss: 1.1360 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/200 - Train: 100%|█| 1/1 [00:00<00:00, 125.02it/s, loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/200]TrainLoss: 0.5952 |ValLoss: 1.1613 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/200 - Train: 100%|█| 1/1 [00:00<00:00, 125.01it/s, loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/200]TrainLoss: 0.5756 |ValLoss: 1.1872 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 - Train: 100%|█| 1/1 [00:00<00:00, 82.38it/s, loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200]TrainLoss: 0.5686 |ValLoss: 1.2133 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/200 - Train: 100%|█| 1/1 [00:00<00:00, 107.19it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/200]TrainLoss: 0.5612 |ValLoss: 1.2395 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/200 - Train: 100%|█| 1/1 [00:00<00:00, 99.94it/s, loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/200]TrainLoss: 0.5456 |ValLoss: 1.2655 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/200 - Train: 100%|█| 1/1 [00:00<00:00, 111.08it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/200]TrainLoss: 0.5341 |ValLoss: 1.2916 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/200 - Train: 100%|█| 1/1 [00:00<00:00, 100.01it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/200]TrainLoss: 0.5227 |ValLoss: 1.3179 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/200 - Train: 100%|█| 1/1 [00:00<00:00, 125.03it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/200]TrainLoss: 0.5063 |ValLoss: 1.3444 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/200 - Train: 100%|█| 1/1 [00:00<00:00, 125.02it/s, loss=0."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/200]TrainLoss: 0.4959 |ValLoss: 1.3706 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/200 - Train: 100%|█| 1/1 [00:00<00:00, 142.82it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/200]TrainLoss: 0.4837 |ValLoss: 1.3968 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/200 - Train: 100%|█| 1/1 [00:00<00:00, 134.97it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/200]TrainLoss: 0.4776 |ValLoss: 1.4231 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/200 - Train: 100%|█| 1/1 [00:00<00:00, 111.11it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/200]TrainLoss: 0.4716 |ValLoss: 1.4490 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/200 - Train: 100%|█| 1/1 [00:00<00:00, 112.81it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200]TrainLoss: 0.4604 |ValLoss: 1.4743 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/200 - Train: 100%|█| 1/1 [00:00<00:00, 142.81it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/200]TrainLoss: 0.4527 |ValLoss: 1.4989 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/200 - Train: 100%|█| 1/1 [00:00<00:00, 118.35it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/200]TrainLoss: 0.4478 |ValLoss: 1.5233 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/200 - Train: 100%|█| 1/1 [00:00<00:00, 118.73it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/200]TrainLoss: 0.4355 |ValLoss: 1.5473 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/200 - Train: 100%|█| 1/1 [00:00<00:00, 90.88it/s, loss=0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/200]TrainLoss: 0.4382 |ValLoss: 1.5709 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/200 - Train: 100%|█| 1/1 [00:00<00:00, 128.38it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/200]TrainLoss: 0.4298 |ValLoss: 1.5938 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/200 - Train: 100%|█| 1/1 [00:00<00:00, 124.96it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/200]TrainLoss: 0.4144 |ValLoss: 1.6163 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/200 - Train: 100%|█| 1/1 [00:00<00:00, 139.10it/s, loss=0."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/200]TrainLoss: 0.4138 |ValLoss: 1.6378 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/200 - Train: 100%|█| 1/1 [00:00<00:00, 142.82it/s, loss=0."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/200]TrainLoss: 0.4055 |ValLoss: 1.6590 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/200 - Train: 100%|█| 1/1 [00:00<00:00, 146.40it/s, loss=0."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/200]TrainLoss: 0.4047 |ValLoss: 1.6798 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/200 - Train: 100%|█| 1/1 [00:00<00:00, 194.28it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/200]TrainLoss: 0.4020 |ValLoss: 1.6997 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/200 - Train: 100%|█| 1/1 [00:00<00:00, 142.86it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/200]TrainLoss: 0.3912 |ValLoss: 1.7188 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/200 - Train: 100%|█| 1/1 [00:00<00:00, 142.86it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/200]TrainLoss: 0.3816 |ValLoss: 1.7371 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/200 - Train: 100%|█| 1/1 [00:00<00:00, 249.99it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/200]TrainLoss: 0.3778 |ValLoss: 1.7549 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/200 - Train: 100%|█████| 1/1 [00:00<?, ?it/s, loss=0.3681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/200]TrainLoss: 0.3681 |ValLoss: 1.7727 |ValAcc: 0.0000|Time: 0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/200 - Train: 100%|█████| 1/1 [00:00<?, ?it/s, loss=0.3764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/200]TrainLoss: 0.3764 |ValLoss: 1.7903 |ValAcc: 0.0000|Time: 0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/200 - Train: 100%|█████| 1/1 [00:00<?, ?it/s, loss=0.3653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/200]TrainLoss: 0.3653 |ValLoss: 1.8070 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/200 - Train: 100%|█| 1/1 [00:00<00:00, 62.26it/s, loss=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/200]TrainLoss: 0.3566 |ValLoss: 1.8234 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/200 - Train: 100%|█| 1/1 [00:00<00:00, 65.19it/s, loss=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/200]TrainLoss: 0.3533 |ValLoss: 1.8391 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/200 - Train: 100%|█| 1/1 [00:00<00:00, 62.19it/s, loss=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/200]TrainLoss: 0.3570 |ValLoss: 1.8537 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/200 - Train: 100%|█| 1/1 [00:00<00:00, 62.78it/s, loss=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/200]TrainLoss: 0.3514 |ValLoss: 1.8674 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/200 - Train: 100%|█| 1/1 [00:00<00:00, 62.51it/s, loss=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/200]TrainLoss: 0.3446 |ValLoss: 1.8808 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/200 - Train: 100%|█████| 1/1 [00:00<?, ?it/s, loss=0.3301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/200]TrainLoss: 0.3301 |ValLoss: 1.8940 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/200 - Train: 100%|█████| 1/1 [00:00<?, ?it/s, loss=0.3363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/200]TrainLoss: 0.3363 |ValLoss: 1.9069 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/200 - Train: 100%|█████| 1/1 [00:00<?, ?it/s, loss=0.3244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/200]TrainLoss: 0.3244 |ValLoss: 1.9195 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/200 - Train: 100%|█████| 1/1 [00:00<?, ?it/s, loss=0.3264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/200]TrainLoss: 0.3264 |ValLoss: 1.9314 |ValAcc: 0.0000|Time: 0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/200 - Train: 100%|█████| 1/1 [00:00<?, ?it/s, loss=0.3202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/200]TrainLoss: 0.3202 |ValLoss: 1.9428 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/200 - Train: 100%|█| 1/1 [00:00<00:00, 326.30it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/200]TrainLoss: 0.3075 |ValLoss: 1.9542 |ValAcc: 0.0000|Time: 0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/200 - Train: 100%|█| 1/1 [00:00<00:00, 62.16it/s, loss=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/200]TrainLoss: 0.3167 |ValLoss: 1.9651 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/200 - Train: 100%|█| 1/1 [00:00<00:00, 63.59it/s, loss=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/200]TrainLoss: 0.3058 |ValLoss: 1.9756 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/200 - Train: 100%|█| 1/1 [00:00<00:00, 62.05it/s, loss=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/200]TrainLoss: 0.3007 |ValLoss: 1.9857 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/200 - Train: 100%|█| 1/1 [00:00<00:00, 86.74it/s, loss=0.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/200]TrainLoss: 0.3056 |ValLoss: 1.9954 |ValAcc: 0.0000|Time: 0.02s\n",
      "Early stopping triggered.\n",
      "Selesai training fine-tuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CELL 9: INISIALISASI dan EKSEKUSI MODEL dan TRAINING\n",
    "\n",
    "# (dari cell 4 : Build Frozen Parts)\n",
    "variant = 'A' # BAGIAN INI BISA DIGANTI A, B, atau C\n",
    "encoder_frozen, transformer_frozen, proj_head_frozen = build_frozen_parts_from_best_pretrained_model(variant, device)\n",
    "print(\"Komponen pra-trained telah dimuat dan dibekukan.\")\n",
    "\n",
    "# (dari cell 5 : Classifier)\n",
    "# Buat instance model FT\n",
    "model = FTClassifier(encoder_frozen, transformer_frozen, proj_head_frozen, num_classes=2).to(device)\n",
    "# Pastikan hanya parameter classifier yang requires_grad=True\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(\"Jumlah parameter yang dilatih (harus hanya classifier):\", sum(p.numel() for p in trainable_params))\n",
    "\n",
    "# (dari cell 7 : Training Loop)\n",
    "# Jalankan training\n",
    "train_finetune(model, variant, train_loader, val_loader, device, num_epochs=200, lr=1e-3, weight_decay=1e-4, patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b047966-990a-4516-8225-4f0fd6565ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best fine-tuned classifier from best_finetuned_ver3A.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_14020\\850362123.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  svb = torch.load(saved_best_finetuned_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# CELL 10: MUAT bobot classifier terbaik (jika ada)\n",
    "\n",
    "saved_best_finetuned_path = f\"best_finetuned_ver3{variant}.pt\"\n",
    "\n",
    "if os.path.exists(saved_best_finetuned_path):\n",
    "    svb = torch.load(saved_best_finetuned_path, map_location=device)\n",
    "    model.classifier.load_state_dict(svb[\"classifier_state\"])\n",
    "    print(\"Loaded best fine-tuned classifier from\", saved_best_finetuned_path)\n",
    "else:\n",
    "    print(\"Tidak ditemukan fine-tuned checkpoint. Pastikan training selesai dan file tersimpan.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1222940-ce7a-4e8a-b120-3feba95eaa49",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5643575a-4014-44d8-8fcd-32f593c91b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: INFERENSI PETA KLASIFIKASI FULL SIZE (GPU, RAM-EFFICIENT, PROB + ARGMAX)\n",
    "\n",
    "def infer_full_map(full_image, model, device, patch_size=9, batch_size=512,\n",
    "                   pad_mode='constant', pad_value=0):\n",
    "    \"\"\"\n",
    "    full_image : numpy array (H, W, B=224)\n",
    "    model      : model fine-tuned (sudah .eval() & .to(device))\n",
    "    device     : torch.device(\"cuda\") atau cpu, tetapi default diisi cuda\n",
    "    patch_size : default 9 (harus sama dengan training)\n",
    "    batch_size : default 512 (bisa dinaik-turunkan)\n",
    "    pad_mode   : 'constant' di sini\n",
    "    pad_value  : 0 untuk zero-padding\n",
    "\n",
    "    return: pred_map (H, W) int {0,1}, prob_map (H, W) float (prob kelas 1)\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    model.eval()\n",
    "    H, W, B = full_image.shape\n",
    "    assert B == 224, \"Expected 224 bands.\"\n",
    "\n",
    "    # 1) Zero pad seluruh citra\n",
    "    half = patch_size // 2\n",
    "    padded = np.pad(\n",
    "        full_image,\n",
    "        pad_width=((half, half), (half, half), (0, 0)),\n",
    "        mode=pad_mode,\n",
    "        constant_values=pad_value\n",
    "    )\n",
    "\n",
    "    # 2) Siapkan output final\n",
    "    pred_map = np.zeros((H, W), dtype=np.uint8)\n",
    "    prob_map = np.zeros((H, W), dtype=np.float32)\n",
    "\n",
    "    # 3) Sliding-window dalam batch kecil, hemat RAM\n",
    "    coords_batch = []\n",
    "    patches_batch = []\n",
    "\n",
    "    for i in tqdm(range(H), desc=\"Inferensi full map (row streaming)\"):\n",
    "        for j in range(W):\n",
    "            # Ambil 1 patch (9x9x224) di posisi pusat (i,j)\n",
    "            pi, pj = i + half, j + half\n",
    "            patch = padded[pi-half:pi+half+1, pj-half:pj+half+1, :]  # (9,9,224)\n",
    "\n",
    "            patches_batch.append(patch)\n",
    "            coords_batch.append((i, j))\n",
    "\n",
    "            # Jika sudah penuh 1 batch atau sudah titik akhir\n",
    "            if len(patches_batch) == batch_size or (i == H-1 and j == W-1):\n",
    "                # Convert ke tensor Conv3D format (B,1,224,9,9)\n",
    "                xb = torch.tensor(patches_batch, dtype=torch.float32).unsqueeze(1)\n",
    "                xb = xb.permute(0,1,4,2,3).to(device)  # (B,1,224,9,9)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    logits = model(xb)                    # (B,2)\n",
    "                    probs  = torch.softmax(logits, dim=1) # (B,2)\n",
    "                    pred   = logits.argmax(dim=1).cpu().numpy()        # int 0/1\n",
    "                    prob1  = probs[:,1].cpu().numpy()                  # kelas oil\n",
    "\n",
    "                for (ci, cj), p, pr in zip(coords_batch, pred, prob1):\n",
    "                    pred_map[ci, cj] = p\n",
    "                    prob_map[ci, cj] = pr\n",
    "\n",
    "                patches_batch.clear()\n",
    "                coords_batch.clear()\n",
    "\n",
    "    return pred_map, prob_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c9b3c88-aa5f-4f5e-a7a6-f077e59a21ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GM01 shapes -> img: (1243, 684, 224) | gt: (1243, 684)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferensi full map (row streaming):   0%| | 0/1243 [00:00<?, ?it/s]C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_14020\\2726772786.py:53: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  xb = torch.tensor(patches_batch, dtype=torch.float32).unsqueeze(1)\n",
      "Inferensi full map (row streaming): 100%|█| 1243/1243 [40:38<00:00,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peta prediksi GM01 disimpan ke pred_map_GM01.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CELL 12: MUAT GM01.mat -> JALANKAN INFERENCE -> SIMPAN PETA\n",
    "\n",
    "# mat_path = \"D:/CurrentlyActiveResearch/oilspill_project/data/raw/GM01.mat\" # absolute path\n",
    "mat_path = \"../data/raw/GM01.mat\" # relative path\n",
    "assert os.path.exists(mat_path), f\"File GM01.mat tidak ditemukan di {mat_path}\"\n",
    "\n",
    "mat = sio.loadmat(mat_path)\n",
    "img = mat[\"img\"]    # (H, W, B)\n",
    "gt_map = mat[\"map\"] # (H, W)\n",
    "\n",
    "print(\"GM01 shapes -> img:\", img.shape, \"| gt:\", gt_map.shape)\n",
    "\n",
    "# Jalankan inference (Peringatan : Proses ini mungkin memakan memori & waktu)\n",
    "pred_map_gm = infer_full_map(img, model, device, patch_size=9, batch_size=512)\n",
    "\n",
    "# Simpan peta prediksi\n",
    "save_dir = \"../data/result/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(os.path.join(save_dir, f\"pred_map_GM01_ver3{variant}.npy\"), pred_map_gm) # pred map untuk file GM01 menyesuaikan ke versi 3A, 3B, atau 3C\n",
    "\n",
    "print(\"Peta prediksi GM01 disimpan ke folder data/result/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a1314df-f997-47ef-91af-4b481c74db99",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [850212, 1700424]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pred_map\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     11\u001b[0m y_true \u001b[38;5;241m=\u001b[39m gt\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m---> 13\u001b[0m oa \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m f1_per_class \u001b[38;5;241m=\u001b[39m f1_score(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# per class\u001b[39;00m\n\u001b[0;32m     15\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(y_true, y_pred)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    217\u001b[0m     ):\n\u001b[1;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    228\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\sklearn\\metrics\\_classification.py:359\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m    358\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n\u001b[1;32m--> 359\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\sklearn\\metrics\\_classification.py:97\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     96\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m---> 97\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     99\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\sklearn\\utils\\validation.py:473\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    471\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    474\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    475\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    476\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [850212, 1700424]"
     ]
    }
   ],
   "source": [
    "# CELL 13: EVALUASI PETA\n",
    "\n",
    "saved_pred_map_path = os.path.join(save_dir, \"pred_map_GM01.npy\")\n",
    "assert os.path.exists(saved_pred_map_path), f\"File pred_map_GM01.npy tidak ditemukan\"\n",
    "\n",
    "pred_map = np.load(saved_pred_map_path)\n",
    "gt = gt_map.astype(int)\n",
    "\n",
    "# Flatten untuk metrik\n",
    "y_pred = pred_map.flatten()\n",
    "y_true = gt.flatten()\n",
    "\n",
    "oa = accuracy_score(y_true, y_pred)\n",
    "f1_per_class = f1_score(y_true, y_pred, average=None)  # per class\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Overall Accuracy (OA):\", oa)\n",
    "print(\"F1 per kelas:\", f1_per_class)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "print(\"\\nReport klasifikasi (per-class precision/recall/f1):\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56349420-e879-4cad-9c8b-9a665cce9adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 14: VISUALISASI PETA PREDIKSI dan GT\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Citra (band visualisasi contoh)\")\n",
    "# untuk visual: ambil 3 pita (mis. 30, 20, 10)\n",
    "b1, b2, b3 = 30, 20, 10\n",
    "rgb = img[:,:, [b1,b2,b3]]\n",
    "# normalisasi untuk tampil\n",
    "rgb_norm = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n",
    "plt.imshow(rgb_norm)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Ground Truth GM01\")\n",
    "plt.imshow(gt, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Prediksi GM01\")\n",
    "plt.imshow(pred_map_gm01, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
