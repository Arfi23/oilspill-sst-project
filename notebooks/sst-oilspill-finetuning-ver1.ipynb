{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8889d4c2-d7e5-4360-a0cd-45a6c808159b",
   "metadata": {},
   "source": [
    "## __Fine-Tuning Versi 1__\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e1dbd9-7962-45af-a399-def539f17363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggunakan device: cuda\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: IMPORT STANDARD & ATUR DEVICE\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Sklearn untuk metrik\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, classification_report\n",
    "\n",
    "# Atur device (periksa GPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Menggunakan device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e25eb4-deca-4904-9a53-0215eb2d9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: IMPLEMENTASI zeroPadding_3D\n",
    "\n",
    "def zeroPadding_3D(old_matrix, pad_length, pad_depth=0):\n",
    "    \"\"\"\n",
    "    old_matrix: numpy array (H, W, B)\n",
    "    pad_length: jumlah pad di spatial (keempat arah)\n",
    "    pad_depth: optional, (default 0)\n",
    "    \"\"\"\n",
    "    new_matrix = np.pad(old_matrix, ((pad_length, pad_length), (pad_length, pad_length), (pad_depth, pad_depth)),\n",
    "                        mode='constant', constant_values=0)\n",
    "    return new_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cb2ba74-2050-4e9c-bfc6-7b0b4148fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: DEFINISI MODEL (encoder, transformer, projection head)\n",
    "\n",
    "class SpectralSpatialEncoder3D(nn.Module):\n",
    "    def __init__(self, embedding_dim=256, init_channels=32):\n",
    "        super().__init__()\n",
    "        # Konvolusi pertama (non-overlapping subpatch)\n",
    "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=init_channels,\n",
    "                               kernel_size=(20,3,3), stride=(20,3,3), padding=0)\n",
    "        self.bn1 = nn.BatchNorm3d(init_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        # Konvolusi kedua: linear projection ke embedding_dim\n",
    "        self.conv2 = nn.Conv3d(in_channels=init_channels, out_channels=embedding_dim,\n",
    "                               kernel_size=(1,1,1), stride=(1,1,1), padding=0)\n",
    "        self.bn2 = nn.BatchNorm3d(embedding_dim)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,1,224,9,9)\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))   # -> (B, init_ch, 11, 3, 3)\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))   # -> (B, 256, 11, 3, 3)\n",
    "        B, C, D, H, W = x.shape\n",
    "        # Permute dan flatten token axis -> (B, 99, 256)\n",
    "        x = x.permute(0,2,3,4,1).contiguous().view(x.size(0), -1, x.size(1))\n",
    "        return x\n",
    "\n",
    "class SimpleTransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_heads=8, num_layers=5, mlp_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
    "                                           dim_feedforward=mlp_dim, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,99,256) -> keluar (B,99,256)\n",
    "        return self.transformer(x)\n",
    "\n",
    "class ProjectionHead_A(nn.Module): # Projection Head VERSI A\n",
    "    def __init__(self, in_dim=256, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(in_dim, proj_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,99,256)\n",
    "        x = x.mean(dim=1)   # Global average pooling antar token -> (B,256)\n",
    "        return self.net(x)  # -> (B,128)\n",
    "    \n",
    "class ProjectionHead_B(nn.Module): # Projection Head VERSI B\n",
    "    def __init__(self, in_dim=256, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(in_dim, proj_dim)\n",
    "\n",
    "    def forward(self, x):      # x: (B, 99, 256)\n",
    "        x = self.net(x)        #  (B, 99, 128)  # proyeksi per-token\n",
    "        x = x.mean(dim=1)      #  (B, 128)      # pooling global antar token\n",
    "        return x\n",
    "\n",
    "class ProjectionHead_C(nn.Module): # Projection Head VERSI C\n",
    "    def __init__(self, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(99, proj_dim)  # 99 ke 128\n",
    "\n",
    "    def forward(self, x):  # x: (B, 99, 256)\n",
    "        x = x.mean(dim=2)        # (B, 99, 1)  # GAP Dalam Token\n",
    "        x = x.squeeze(-1)        # (B, 99)\n",
    "        return self.net(x)        # (B, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d8088b6-975a-4642-93ce-f4c5b42566fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: LOAD HASIL PRETRAINING dan FREEZE semua kecuali classifier nanti\n",
    "\n",
    "def build_frozen_parts_from_best_pretrained_model(variant, device):\n",
    "    \"\"\"\n",
    "    Mengembalikan tiga komponen yang sudah dimuat:\n",
    "    encoder, transformer, proj_head (semua parameter dibekukan)\n",
    "    \"\"\"\n",
    "    # instantiate model parts\n",
    "    encoder = SpectralSpatialEncoder3D(embedding_dim=256).to(device)\n",
    "    transformer = SimpleTransformerEncoder(embed_dim=256).to(device)\n",
    "\n",
    "    if variant == 'A':\n",
    "        proj_head = ProjectionHead_A(in_dim=256, proj_dim=128).to(device)\n",
    "        best_model_path = f\"best_sst_ver3{variant}.pt\" # ini file pretrained best model ver3A\n",
    "    elif variant == 'B' :\n",
    "        proj_head = ProjectionHead_B(in_dim=256, proj_dim=128).to(device)\n",
    "        best_model_path = f\"best_sst_ver3{variant}.pt\" # ini file pretrained best model ver3B\n",
    "    elif variant == 'C':\n",
    "        proj_head = ProjectionHead_C(proj_dim=128).to(device)\n",
    "        best_model_path = f\"best_sst_ver3{variant}.pt\" # ini file pretrained best model ver3C\n",
    "    else:\n",
    "        raise ValueError(\"variant must be 'A'/'B'/'C'\")\n",
    "\n",
    "    # load best model yang sudah dilatih sebelumnya\n",
    "    assert os.path.exists(best_model_path), f\"Best model tidak ditemukan: {best_model_path}\"\n",
    "    bm_point = torch.load(best_model_path, map_location=device) #bm_point untuk menampung best model yang di-load\n",
    "\n",
    "    # muat state dict (jaga kompatibilitas)\n",
    "    if \"encoder_state\" in bm_point:\n",
    "        encoder.load_state_dict(bm_point[\"encoder_state\"])\n",
    "    if \"transformer_state\" in bm_point:\n",
    "        transformer.load_state_dict(bm_point[\"transformer_state\"])\n",
    "    if \"proj_head_state\" in bm_point:\n",
    "        try:\n",
    "            proj_head.load_state_dict(bm_point[\"proj_head_state\"])\n",
    "        except Exception as e:\n",
    "            # coba non-strict load bila ada mismatch minor\n",
    "            print(\"[PERINGATAN] proj_head.load_state_dict error -> mencoba strict=False. Error:\", e)\n",
    "            proj_head.load_state_dict(bm_point[\"proj_head_state\"], strict=False)\n",
    "\n",
    "    # Freeze param agar tidak ikut update saat fine-tuning\n",
    "    for p in encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in transformer.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in proj_head.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    return encoder, transformer, proj_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea4bc443-29d8-4c0e-bb64-d5c854125d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: FTClassifier (hanya classifier yang trainable)\n",
    "\n",
    "class FTClassifier(nn.Module):\n",
    "    def __init__(self, encoder, transformer, proj_head, num_classes=2):\n",
    "        super().__init__()\n",
    "        # komponen beku (sudah di-freeze sebelumnya)\n",
    "        self.encoder = encoder\n",
    "        self.transformer = transformer\n",
    "        self.proj_head = proj_head\n",
    "        # classifier linear sederhana\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Semua feature extraction dilakukan tanpa grad untuk menghemat memori\n",
    "        with torch.no_grad():\n",
    "            feat = self.encoder(x)        # (B,99,256)\n",
    "            feat = self.transformer(feat) # (B,99,256)\n",
    "            proj = self.proj_head(feat)   # (B,128)\n",
    "        logits = self.classifier(proj)    # (B,2)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079df931-97d9-4359-ba9a-6558e3eef266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: HELPERS - memuat dataset patch\n",
    "\n",
    "def load_patch_dataset(data_dir=\"../data/processed\", batch_size=32, val_split=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Memuat patch_class0.npy dan patch_class1.npy,\n",
    "    menggabungkan, kemudian membagi ke train/val\n",
    "    Output: train_loader, val_loader\n",
    "    \"\"\"\n",
    "    path0 = os.path.join(data_dir, \"patch_class0.npy\")\n",
    "    path1 = os.path.join(data_dir, \"patch_class1.npy\")\n",
    "    assert os.path.exists(path0) and os.path.exists(path1), \"File patch_class*.npy tidak ditemukan\"\n",
    "\n",
    "    p0 = np.load(path0)  # shape (N0, 9,9,224)\n",
    "    p1 = np.load(path1)  # shape (N1, 9,9,224)\n",
    "    X_all = np.concatenate([p0, p1], axis=0)\n",
    "    y_all = np.concatenate([np.zeros(len(p0)), np.ones(len(p1))], axis=0)\n",
    "\n",
    "    # ubah ke tensor PyTorch format conv3d: (N,1,224,9,9)\n",
    "    X_tensor = torch.tensor(X_all, dtype=torch.float32).unsqueeze(1).permute(0,1,4,2,3)\n",
    "    y_tensor = torch.tensor(y_all, dtype=torch.long)\n",
    "\n",
    "    # split train/val konsisten\n",
    "    N = len(X_tensor)\n",
    "    rng = torch.Generator().manual_seed(seed)\n",
    "    indices = torch.randperm(N, generator=rng)\n",
    "    val_size = int(val_split * N)\n",
    "    val_idx = indices[:val_size]\n",
    "    train_idx = indices[val_size:]\n",
    "\n",
    "    train_ds = TensorDataset(X_tensor[train_idx], y_tensor[train_idx])\n",
    "    val_ds = TensorDataset(X_tensor[val_idx], y_tensor[val_idx])\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "502d0cf6-cf08-4c6b-b24b-9049f9ef0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: TRAINING LOOP untuk fine-tuning classifier (hanya classifier param yang dioptimasi)\n",
    "\n",
    "def train_finetune(model, variant, train_loader, val_loader, device,\n",
    "                   num_epochs=200, lr=1e-3, weight_decay=1e-4, patience=50):\n",
    "    optimizer = optim.AdamW(model.classifier.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve = 0\n",
    "    start_epoch = 1\n",
    "    \n",
    "    \n",
    "    checkpoint_finetuned_path = f\"checkpoint_sst_finetuned_ver3{variant}.pt\" # ini file finetuned checkpoint untuk versi {variant}\n",
    "    best_finetuned_path = f\"best_finetuned_ver3{variant}.pt\" # ini file finetuned best model untuk versi {variant}\n",
    "    \n",
    "    # ==== Jika checkpoint ada, lanjutkan dari sana ====\n",
    "    if os.path.exists(checkpoint_finetuned_path):\n",
    "        checkpoint = torch.load(checkpoint_finetuned_path, map_location=device)\n",
    "        model.classifier.load_state_dict(checkpoint[\"classifier_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "        print(f\"[OK] Checkpoint ditemukan. Melanjutkan dari epoch {start_epoch}.\")\n",
    "    else:\n",
    "        print(\"[MAAF] Tidak ditemukan checkpoint. Memulai training dari awal.\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs+1):\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} - Train\")\n",
    "        for x_batch, y_batch in pbar:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(x_batch)\n",
    "\n",
    "            loss = criterion(logits, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * x_batch.size(0)\n",
    "            n += x_batch.size(0)\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_train_loss = total_loss / n\n",
    "\n",
    "        # validasi\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        nval = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for xv, yv in val_loader:\n",
    "                xv = xv.to(device); yv = yv.to(device)\n",
    "                logits = model(xv)\n",
    "                lossv = criterion(logits, yv)\n",
    "                val_loss += lossv.item() * xv.size(0)\n",
    "                nval += xv.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == yv).sum().item()\n",
    "        avg_val_loss = val_loss / nval\n",
    "        val_acc = correct / nval\n",
    "\n",
    "        # Waktu per epoch\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}]\" \n",
    "              f\"TrainLoss: {avg_train_loss:.4f} |\"\n",
    "              f\"ValLoss: {avg_val_loss:.4f} |\" \n",
    "              f\"ValAcc: {val_acc:.4f}|\"\n",
    "              f\"Time: {epoch_time:.2f}s\")\n",
    "\n",
    "        checkpoint = {\n",
    "            \"epoch\" : epoch,\n",
    "            \"classifier_state\" : model.classifier.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"best_val_loss\" : best_val_loss\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_finetuned_path)\n",
    "\n",
    "        # checkpoint best classifier\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            no_improve = 0\n",
    "            torch.save(checkpoint, best_finetuned_path)\n",
    "            print(\">> Model fine-tuned terbaik disimpan:\", best_finetuned_path)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(\"Selesai training fine-tuning.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6306b505-85b2-4dbf-96e9-ff1090b55776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah sampel train: 8 | jumlah batch train: 1\n",
      "Jumlah sampel val: 2 | jumlah batch val: 1\n"
     ]
    }
   ],
   "source": [
    "# CELL 8: EKSEKUSI DATA LOADER\n",
    "\n",
    "# (dari cell 6 : Data Loader)\n",
    "# Contoh muat data\n",
    "train_loader, val_loader = load_patch_dataset(data_dir=\"../data/processed\", batch_size=32, val_split=0.2)\n",
    "print(\"Jumlah sampel train:\", sum(len(batch[0]) for batch in train_loader), \"| jumlah batch train:\", len(train_loader))\n",
    "print(\"Jumlah sampel val:\", sum(len(batch[0]) for batch in val_loader), \"| jumlah batch val:\", len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a40127ff-45ea-41ed-85f3-7e1f6eda42ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_16324\\1182809202.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bm_point = torch.load(best_model_path, map_location=device) #bm_point untuk menampung best model yang di-load\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Komponen pra-trained telah dimuat dan dibekukan.\n",
      "Jumlah parameter yang dilatih (harus hanya classifier): 258\n",
      "[MAAF] Tidak ditemukan checkpoint. Memulai training dari awal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 - Train:   0%|                   | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasLtMatmul with transpose_mat1 1 transpose_mat2 0 m 256 n 792 k 256 mat1_ld 256 mat2_ld 256 result_ld 256 abcType 0 computeType 68 scaleType 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJumlah parameter yang dilatih (harus hanya classifier):\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m trainable_params))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# (dari cell 7 : Training Loop)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Jalankan training\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mtrain_finetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 37\u001b[0m, in \u001b[0;36mtrain_finetune\u001b[1;34m(model, variant, train_loader, val_loader, device, num_epochs, lr, weight_decay, patience)\u001b[0m\n\u001b[0;32m     34\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m x_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     35\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 37\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y_batch)\n\u001b[0;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36mFTClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     16\u001b[0m     feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)        \u001b[38;5;66;03m# (B,99,256)\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B,99,256)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_head(feat)   \u001b[38;5;66;03m# (B,128)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(proj)    \u001b[38;5;66;03m# (B,2)\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 35\u001b[0m, in \u001b[0;36mSimpleTransformerEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# x: (B,99,256) -> keluar (B,99,256)\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:511\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    508\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 511\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    519\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:904\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    900\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    902\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[0;32m    903\u001b[0m         x\n\u001b[1;32m--> 904\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    905\u001b[0m     )\n\u001b[0;32m    906\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:918\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sa_block\u001b[39m(\n\u001b[0;32m    912\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    913\u001b[0m     x: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    916\u001b[0m     is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    917\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 918\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1368\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1343\u001b[0m         query,\n\u001b[0;32m   1344\u001b[0m         key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1368\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\oilspill\\lib\\site-packages\\torch\\nn\\functional.py:6285\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   6278\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m scaled_dot_product_attention(\n\u001b[0;32m   6279\u001b[0m     q, k, v, attn_mask, dropout_p, is_causal\n\u001b[0;32m   6280\u001b[0m )\n\u001b[0;32m   6281\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   6282\u001b[0m     attn_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m tgt_len, embed_dim)\n\u001b[0;32m   6283\u001b[0m )\n\u001b[1;32m-> 6285\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6286\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(tgt_len, bsz, attn_output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   6287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[0;32m   6288\u001b[0m     \u001b[38;5;66;03m# squeeze the output if input was unbatched\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasLtMatmul with transpose_mat1 1 transpose_mat2 0 m 256 n 792 k 256 mat1_ld 256 mat2_ld 256 result_ld 256 abcType 0 computeType 68 scaleType 0"
     ]
    }
   ],
   "source": [
    "# CELL 9: INISIALISASI dan EKSEKUSI MODEL dan TRAINING\n",
    "\n",
    "# (dari cell 4 : Build Frozen Parts)\n",
    "variant = 'A' # BAGIAN INI BISA DIGANTI A, B, atau C\n",
    "encoder_frozen, transformer_frozen, proj_head_frozen = build_frozen_parts_from_best_pretrained_model(variant, device)\n",
    "print(\"Komponen pra-trained telah dimuat dan dibekukan.\")\n",
    "\n",
    "# (dari cell 5 : Classifier)\n",
    "# Buat instance model FT\n",
    "model = FTClassifier(encoder_frozen, transformer_frozen, proj_head_frozen, num_classes=2).to(device)\n",
    "# Pastikan hanya parameter classifier yang requires_grad=True\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(\"Jumlah parameter yang dilatih (harus hanya classifier):\", sum(p.numel() for p in trainable_params))\n",
    "\n",
    "# (dari cell 7 : Training Loop)\n",
    "# Jalankan training\n",
    "train_finetune(model, variant, train_loader, val_loader, device, num_epochs=200, lr=1e-3, weight_decay=1e-4, patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b047966-990a-4516-8225-4f0fd6565ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: MUAT bobot classifier terbaik (jika ada)\n",
    "\n",
    "saved_best_finetuned_path = f\"best_finetuned_ver3{variant}.pt\"\n",
    "\n",
    "if os.path.exists(saved_best_finetuned_path):\n",
    "    svb = torch.load(saved_best_finetuned_path, map_location=device)\n",
    "    model.classifier.load_state_dict(svb[\"classifier_state\"])\n",
    "    print(\"Loaded best fine-tuned classifier from\", saved_best_finetuned_path)\n",
    "else:\n",
    "    print(\"Tidak ditemukan fine-tuned checkpoint. Pastikan training selesai dan file tersimpan.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1222940-ce7a-4e8a-b120-3feba95eaa49",
   "metadata": {},
   "source": [
    "============================== \n",
    "just separator\n",
    "=============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5643575a-4014-44d8-8fcd-32f593c91b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: INFERENCE PETA KLASIFIKASI (SLIDING WINDOW PATCH-CENTER) - versi batch untuk efisiensi\n",
    "\n",
    "def inference_map_patch_center(full_image, model, device, patch_size=9, batch_size=256, pad_mode='zero'):\n",
    "    \"\"\"\n",
    "    full_image: numpy (H, W, B)\n",
    "    model: model FTClassifier yang sudah dimuat bobot classifier terbaik\n",
    "    patch_size: 9 (patch spatial)\n",
    "    pad_mode: hanya info; yang akan digunakan adalah zero padding (zeroPadding_3D)\n",
    "    return: pred_map (H, W) int {0,1}\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    H, W, B = full_image.shape\n",
    "    assert B == 224, \"Diharapkan 224 band; sesuaikan bila berbeda.\"\n",
    "\n",
    "    half = patch_size // 2\n",
    "    # Gunakan zero padding (seperti pada preprocessing)\n",
    "    padded = zeroPadding_3D(full_image, half)  # hasil shape (H+2*half, W+2*half, B)\n",
    "\n",
    "    # Pre-buat semua patch dalam bentuk tumpukan (agar memudahkan batching)\n",
    "    coords = []\n",
    "    patches = []\n",
    "    # iterasi per piksel pusat\n",
    "    for i in range(half, half + H):\n",
    "        for j in range(half, half + W):\n",
    "            patch = padded[i-half:i+half+1, j-half:j+half+1, :]  # (9,9,224)\n",
    "            patches.append(patch)\n",
    "            coords.append((i-half, j-half))\n",
    "\n",
    "    patches = np.stack(patches, axis=0)  # shape (H*W, 9,9,224)\n",
    "    N = patches.shape[0]\n",
    "\n",
    "    # Konversi ke tensor conv3d format (N,1,224,9,9)\n",
    "    X = torch.tensor(patches, dtype=torch.float32).unsqueeze(1).permute(0,1,4,2,3)\n",
    "\n",
    "    loader = DataLoader(TensorDataset(X, torch.zeros(len(X), dtype=torch.long)),\n",
    "                        batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in tqdm(loader, desc=\"Inferensi peta (batch)\"):\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)            # (B,2)\n",
    "            p = logits.argmax(dim=1).cpu().numpy()\n",
    "            preds.append(p)\n",
    "    preds = np.concatenate(preds, axis=0)  # (H*W, )\n",
    "\n",
    "    pred_map = preds.reshape(H, W)\n",
    "    return pred_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b3c88-aa5f-4f5e-a7a6-f077e59a21ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 12: MUAT GM01.mat -> JALANKAN INFERENCE -> SIMPAN PETA\n",
    "\n",
    "# mat_path = \"D:/CurrentlyActiveResearch/oilspill_project/data/raw/GM01.mat\" # absolute path\n",
    "mat_path = \"../data/raw/GM01.mat\" # relative path\n",
    "assert os.path.exists(mat_path), f\"File GM01.mat tidak ditemukan di {mat_path}\"\n",
    "\n",
    "mat = sio.loadmat(mat_path)\n",
    "img = mat[\"img\"]    # (H, W, B)\n",
    "gt_map = mat[\"map\"] # (H, W)\n",
    "\n",
    "print(\"GM01 shapes -> img:\", img.shape, \"| gt:\", gt_map.shape)\n",
    "\n",
    "# Jalankan inference (Peringatan : Proses ini mungkin memakan memori & waktu)\n",
    "pred_map_gm01 = inference_map_patch_center(img, model, device, patch_size=9, batch_size=512)\n",
    "\n",
    "# Simpan peta prediksi\n",
    "save_dir = \"../data/result/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(os.path.join(save_dir, \"pred_map_GM01.npy\"), pred_map_gm01)\n",
    "\n",
    "print(\"Peta prediksi GM01 disimpan ke pred_map_GM01.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1314df-f997-47ef-91af-4b481c74db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13: EVALUASI PETA\n",
    "\n",
    "saved_pred_map_path = os.path.join(save_dir, \"pred_map_GM01.npy\")\n",
    "assert os.path.exists(saved_pred_map_path), f\"File pred_map_GM01.npy tidak ditemukan\"\n",
    "\n",
    "pred_map = np.load(saved_pred_map_path)\n",
    "gt = gt_map.astype(int)\n",
    "\n",
    "# Flatten untuk metrik\n",
    "y_pred = pred_map.flatten()\n",
    "y_true = gt.flatten()\n",
    "\n",
    "oa = accuracy_score(y_true, y_pred)\n",
    "f1_per_class = f1_score(y_true, y_pred, average=None)  # per class\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Overall Accuracy (OA):\", oa)\n",
    "print(\"F1 per kelas:\", f1_per_class)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "print(\"\\nReport klasifikasi (per-class precision/recall/f1):\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56349420-e879-4cad-9c8b-9a665cce9adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 14: VISUALISASI PETA PREDIKSI dan GT\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Citra (band visualisasi contoh)\")\n",
    "# untuk visual: ambil 3 pita (mis. 30, 20, 10)\n",
    "b1, b2, b3 = 30, 20, 10\n",
    "rgb = img[:,:, [b1,b2,b3]]\n",
    "# normalisasi untuk tampil\n",
    "rgb_norm = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n",
    "plt.imshow(rgb_norm)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Ground Truth GM01\")\n",
    "plt.imshow(gt, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Prediksi GM01\")\n",
    "plt.imshow(pred_map_gm01, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
