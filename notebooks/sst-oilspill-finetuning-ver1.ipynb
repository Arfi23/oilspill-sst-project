{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8889d4c2-d7e5-4360-a0cd-45a6c808159b",
   "metadata": {},
   "source": [
    "## __Fine-Tuning Versi 1__\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e1dbd9-7962-45af-a399-def539f17363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menggunakan device: cuda\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: IMPORT STANDARD & ATUR DEVICE\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Sklearn untuk metrik\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, classification_report\n",
    "\n",
    "# Atur device (periksa GPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Menggunakan device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e25eb4-deca-4904-9a53-0215eb2d9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: IMPLEMENTASI zeroPadding_3D\n",
    "\n",
    "def zeroPadding_3D(old_matrix, pad_length, pad_depth=0):\n",
    "    \"\"\"\n",
    "    old_matrix: numpy array (H, W, B)\n",
    "    pad_length: jumlah pad di spatial (keempat arah)\n",
    "    pad_depth: optional, (default 0)\n",
    "    \"\"\"\n",
    "    new_matrix = np.pad(old_matrix, ((pad_length, pad_length), (pad_length, pad_length), (pad_depth, pad_depth)),\n",
    "                        mode='constant', constant_values=0)\n",
    "    return new_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cb2ba74-2050-4e9c-bfc6-7b0b4148fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: DEFINISI MODEL (encoder, transformer, projection head)\n",
    "\n",
    "class SpectralSpatialEncoder3D(nn.Module):\n",
    "    def __init__(self, embedding_dim=256, init_channels=32):\n",
    "        super().__init__()\n",
    "        # Konvolusi pertama (non-overlapping subpatch)\n",
    "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=init_channels,\n",
    "                               kernel_size=(20,3,3), stride=(20,3,3), padding=0)\n",
    "        self.bn1 = nn.BatchNorm3d(init_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        # Konvolusi kedua: linear projection ke embedding_dim\n",
    "        self.conv2 = nn.Conv3d(in_channels=init_channels, out_channels=embedding_dim,\n",
    "                               kernel_size=(1,1,1), stride=(1,1,1), padding=0)\n",
    "        self.bn2 = nn.BatchNorm3d(embedding_dim)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,1,224,9,9)\n",
    "        x = self.relu1(self.bn1(self.conv1(x)))   # -> (B, init_ch, 11, 3, 3)\n",
    "        x = self.relu2(self.bn2(self.conv2(x)))   # -> (B, 256, 11, 3, 3)\n",
    "        B, C, D, H, W = x.shape\n",
    "        # Permute dan flatten token axis -> (B, 99, 256)\n",
    "        x = x.permute(0,2,3,4,1).contiguous().view(x.size(0), -1, x.size(1))\n",
    "        return x\n",
    "\n",
    "class SimpleTransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=256, num_heads=8, num_layers=5, mlp_dim=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
    "                                           dim_feedforward=mlp_dim, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,99,256) -> keluar (B,99,256)\n",
    "        return self.transformer(x)\n",
    "\n",
    "class ProjectionHead_A(nn.Module): # Projection Head VERSI A\n",
    "    def __init__(self, in_dim=256, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(in_dim, proj_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,99,256)\n",
    "        x = x.mean(dim=1)   # Global average pooling antar token -> (B,256)\n",
    "        return self.net(x)  # -> (B,128)\n",
    "    \n",
    "class ProjectionHead_B(nn.Module): # Projection Head VERSI B\n",
    "    def __init__(self, in_dim=256, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(in_dim, proj_dim)\n",
    "\n",
    "    def forward(self, x):      # x: (B, 99, 256)\n",
    "        x = self.net(x)        #  (B, 99, 128)  # proyeksi per-token\n",
    "        x = x.mean(dim=1)      #  (B, 128)      # pooling global antar token\n",
    "        return x\n",
    "\n",
    "class ProjectionHead_C(nn.Module): # Projection Head VERSI C\n",
    "    def __init__(self, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(99, proj_dim)  # 99 ke 128\n",
    "\n",
    "    def forward(self, x):  # x: (B, 99, 256)\n",
    "        x = x.mean(dim=2)        # (B, 99, 1)  # GAP Dalam Token\n",
    "        x = x.squeeze(-1)        # (B, 99)\n",
    "        return self.net(x)        # (B, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d8088b6-975a-4642-93ce-f4c5b42566fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: LOAD HASIL PRETRAINING dan FREEZE semua kecuali classifier nanti\n",
    "\n",
    "def build_frozen_parts_from_best_pretrained_model(variant, device):\n",
    "    \"\"\"\n",
    "    Mengembalikan tiga komponen yang sudah dimuat:\n",
    "    encoder, transformer, proj_head (semua parameter dibekukan)\n",
    "    \"\"\"\n",
    "    # instantiate model parts\n",
    "    encoder = SpectralSpatialEncoder3D(embedding_dim=256).to(device)\n",
    "    transformer = SimpleTransformerEncoder(embed_dim=256).to(device)\n",
    "\n",
    "    if variant == 'A':\n",
    "        proj_head = ProjectionHead_A(in_dim=256, proj_dim=128).to(device)\n",
    "        best_model_path = f\"best_sst_ver3{variant}.pt\" # ini file pretrained best model ver3A\n",
    "    elif variant == 'B' :\n",
    "        proj_head = ProjectionHead_B(in_dim=256, proj_dim=128).to(device)\n",
    "        best_model_path = f\"best_sst_ver3{variant}.pt\" # ini file pretrained best model ver3B\n",
    "    elif variant == 'C':\n",
    "        proj_head = ProjectionHead_C(proj_dim=128).to(device)\n",
    "        best_model_path = f\"best_sst_ver3{variant}.pt\" # ini file pretrained best model ver3C\n",
    "    else:\n",
    "        raise ValueError(\"variant must be 'A'/'B'/'C'\")\n",
    "\n",
    "    # load best model yang sudah dilatih sebelumnya\n",
    "    assert os.path.exists(best_model_path), f\"Best model tidak ditemukan: {best_model_path}\"\n",
    "    bm_point = torch.load(best_model_path, map_location=device) #bm_point untuk menampung best model yang di-load\n",
    "\n",
    "    # muat state dict (jaga kompatibilitas)\n",
    "    if \"encoder_state\" in bm_point:\n",
    "        encoder.load_state_dict(bm_point[\"encoder_state\"])\n",
    "    if \"transformer_state\" in bm_point:\n",
    "        transformer.load_state_dict(bm_point[\"transformer_state\"])\n",
    "    if \"proj_head_state\" in bm_point:\n",
    "        try:\n",
    "            proj_head.load_state_dict(bm_point[\"proj_head_state\"])\n",
    "        except Exception as e:\n",
    "            # coba non-strict load bila ada mismatch minor\n",
    "            print(\"[PERINGATAN] proj_head.load_state_dict error -> mencoba strict=False. Error:\", e)\n",
    "            proj_head.load_state_dict(bm_point[\"proj_head_state\"], strict=False)\n",
    "\n",
    "    # Freeze param agar tidak ikut update saat fine-tuning\n",
    "    for p in encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in transformer.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in proj_head.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Memastikan modul-modul beku dalam mode eval agar BatchNorm/Dropout stabil\n",
    "    encoder.eval()\n",
    "    transformer.eval()\n",
    "    proj_head.eval()\n",
    "\n",
    "    return encoder, transformer, proj_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea4bc443-29d8-4c0e-bb64-d5c854125d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: FTClassifier (hanya classifier yang trainable)\n",
    "\n",
    "class FTClassifier(nn.Module):\n",
    "    def __init__(self, encoder, transformer, proj_head, num_classes=2):\n",
    "        super().__init__()\n",
    "        # komponen beku (sudah di-freeze sebelumnya)\n",
    "        self.encoder = encoder\n",
    "        self.transformer = transformer\n",
    "        self.proj_head = proj_head\n",
    "        # classifier linear sederhana\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Semua feature extraction dilakukan tanpa grad untuk menghemat memori\n",
    "        with torch.no_grad():\n",
    "            feat = self.encoder(x)        # (B,99,256)\n",
    "            feat = self.transformer(feat) # (B,99,256)\n",
    "            proj = self.proj_head(feat)   # (B,128)\n",
    "        logits = self.classifier(proj)    # (B,2)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079df931-97d9-4359-ba9a-6558e3eef266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: HELPERS - memuat dataset patch\n",
    "\n",
    "def load_patch_dataset(data_dir=\"../data/processed\", batch_size=32, val_split=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Memuat patch_class0.npy dan patch_class1.npy,\n",
    "    menggabungkan, kemudian membagi ke train/val\n",
    "    Output: train_loader, val_loader\n",
    "    \"\"\"\n",
    "    path0 = os.path.join(data_dir, \"patch_class0.npy\")\n",
    "    path1 = os.path.join(data_dir, \"patch_class1.npy\")\n",
    "    assert os.path.exists(path0) and os.path.exists(path1), \"File patch_class*.npy tidak ditemukan\"\n",
    "\n",
    "    p0 = np.load(path0)  # shape (N0, 9,9,224)\n",
    "    p1 = np.load(path1)  # shape (N1, 9,9,224)\n",
    "    X_all = np.concatenate([p0, p1], axis=0)\n",
    "    y_all = np.concatenate([np.zeros(len(p0)), np.ones(len(p1))], axis=0)\n",
    "\n",
    "    # ubah ke tensor PyTorch format conv3d: (N,1,224,9,9)\n",
    "    X_tensor = torch.tensor(X_all, dtype=torch.float32).unsqueeze(1).permute(0,1,4,2,3)\n",
    "    y_tensor = torch.tensor(y_all, dtype=torch.long)\n",
    "\n",
    "    # split train/val konsisten\n",
    "    N = len(X_tensor)\n",
    "    rng = torch.Generator().manual_seed(seed)\n",
    "    indices = torch.randperm(N, generator=rng)\n",
    "    val_size = int(val_split * N)\n",
    "    val_idx = indices[:val_size]\n",
    "    train_idx = indices[val_size:]\n",
    "\n",
    "    train_ds = TensorDataset(X_tensor[train_idx], y_tensor[train_idx])\n",
    "    val_ds = TensorDataset(X_tensor[val_idx], y_tensor[val_idx])\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "502d0cf6-cf08-4c6b-b24b-9049f9ef0860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: TRAINING LOOP untuk fine-tuning classifier (hanya classifier param yang dioptimasi)\n",
    "\n",
    "def train_finetune(model, variant, train_loader, val_loader, device,\n",
    "                   num_epochs=200, lr=1e-3, weight_decay=1e-4, patience=50):\n",
    "    optimizer = optim.AdamW(model.classifier.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve = 0\n",
    "    start_epoch = 1\n",
    "    \n",
    "    \n",
    "    checkpoint_finetuned_path = f\"checkpoint_sst_finetuned_ver3{variant}.pt\" # ini file finetuned checkpoint untuk versi {variant}\n",
    "    best_finetuned_path = f\"best_finetuned_ver3{variant}.pt\" # ini file finetuned best model untuk versi {variant}\n",
    "    \n",
    "    # ==== Jika checkpoint ada, lanjutkan dari sana ====\n",
    "    if os.path.exists(checkpoint_finetuned_path):\n",
    "        checkpoint = torch.load(checkpoint_finetuned_path, map_location=device)\n",
    "        model.classifier.load_state_dict(checkpoint[\"classifier_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "        print(f\"[OK] Checkpoint ditemukan. Melanjutkan dari epoch {start_epoch}.\")\n",
    "    else:\n",
    "        print(\"[MAAF] Tidak ditemukan checkpoint. Memulai training dari awal.\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs+1):\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} - Train\")\n",
    "        for x_batch, y_batch in pbar:\n",
    "            # DEBUG (hapus setelah selesai cek)\n",
    "            # if start_epoch == epoch and n == 0:\n",
    "            #     print(\"DEBUG Batch shapes -> x_batch:\", x_batch.shape, \" y_batch:\", y_batch.shape)\n",
    "                \n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(x_batch)\n",
    "\n",
    "            loss = criterion(logits, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * x_batch.size(0)\n",
    "            n += x_batch.size(0)\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_train_loss = total_loss / n\n",
    "\n",
    "        # validasi\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        nval = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for xv, yv in val_loader:\n",
    "                xv = xv.to(device); yv = yv.to(device)\n",
    "                logits = model(xv)\n",
    "                lossv = criterion(logits, yv)\n",
    "                val_loss += lossv.item() * xv.size(0)\n",
    "                nval += xv.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                correct += (preds == yv).sum().item()\n",
    "        avg_val_loss = val_loss / nval\n",
    "        val_acc = correct / nval\n",
    "\n",
    "        # Waktu per epoch\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}]\" \n",
    "              f\"TrainLoss: {avg_train_loss:.4f} |\"\n",
    "              f\"ValLoss: {avg_val_loss:.4f} |\" \n",
    "              f\"ValAcc: {val_acc:.4f}|\"\n",
    "              f\"Time: {epoch_time:.2f}s\")\n",
    "\n",
    "        checkpoint = {\n",
    "            \"epoch\" : epoch,\n",
    "            \"classifier_state\" : model.classifier.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"best_val_loss\" : best_val_loss\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_finetuned_path)\n",
    "\n",
    "        # checkpoint best classifier\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            no_improve = 0\n",
    "            torch.save(checkpoint, best_finetuned_path)\n",
    "            print(\">> Model fine-tuned terbaik disimpan:\", best_finetuned_path)\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(\"Selesai training fine-tuning.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6306b505-85b2-4dbf-96e9-ff1090b55776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah sampel train: 8 | jumlah batch train: 1\n",
      "Jumlah sampel val: 2 | jumlah batch val: 1\n"
     ]
    }
   ],
   "source": [
    "# CELL 8: EKSEKUSI DATA LOADER\n",
    "\n",
    "# (dari cell 6 : Data Loader)\n",
    "# Contoh muat data\n",
    "train_loader, val_loader = load_patch_dataset(data_dir=\"../data/processed\", batch_size=32, val_split=0.2)\n",
    "print(\"Jumlah sampel train:\", sum(len(batch[0]) for batch in train_loader), \"| jumlah batch train:\", len(train_loader))\n",
    "print(\"Jumlah sampel val:\", sum(len(batch[0]) for batch in val_loader), \"| jumlah batch val:\", len(val_loader))\n",
    "\n",
    "# kosongkan cache dan cek memori gpu\n",
    "if device.startswith('cuda'):\n",
    "    torch.cuda.empty_cache()\n",
    "    # !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a40127ff-45ea-41ed-85f3-7e1f6eda42ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_14020\\1638809886.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bm_point = torch.load(best_model_path, map_location=device) #bm_point untuk menampung best model yang di-load\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Komponen pra-trained telah dimuat dan dibekukan.\n",
      "Jumlah parameter yang dilatih (harus hanya classifier): 258\n",
      "[MAAF] Tidak ditemukan checkpoint. Memulai training dari awal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 - Train: 100%|█| 1/1 [00:00<00:00,  4.39it/s, loss=0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200]TrainLoss: 0.7203 |ValLoss: 0.9997 |ValAcc: 0.0000|Time: 0.25s\n",
      ">> Model fine-tuned terbaik disimpan: best_finetuned_ver3A.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 - Train: 100%|█| 1/1 [00:00<00:00, 100.33it/s, loss=0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/200]TrainLoss: 0.7043 |ValLoss: 1.0213 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200 - Train: 100%|█| 1/1 [00:00<00:00, 132.25it/s, loss=0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/200]TrainLoss: 0.6862 |ValLoss: 1.0427 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/200 - Train: 100%|█| 1/1 [00:00<00:00, 182.11it/s, loss=0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/200]TrainLoss: 0.6618 |ValLoss: 1.0652 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 - Train: 100%|█| 1/1 [00:00<00:00, 133.31it/s, loss=0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/200]TrainLoss: 0.6421 |ValLoss: 1.0885 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/200 - Train: 100%|█| 1/1 [00:00<00:00, 333.49it/s, loss=0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/200]TrainLoss: 0.6271 |ValLoss: 1.1120 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/200 - Train: 100%|█| 1/1 [00:00<00:00, 136.46it/s, loss=0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/200]TrainLoss: 0.6121 |ValLoss: 1.1360 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/200 - Train: 100%|█| 1/1 [00:00<00:00, 125.02it/s, loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/200]TrainLoss: 0.5952 |ValLoss: 1.1613 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/200 - Train: 100%|█| 1/1 [00:00<00:00, 125.01it/s, loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/200]TrainLoss: 0.5756 |ValLoss: 1.1872 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 - Train: 100%|█| 1/1 [00:00<00:00, 82.38it/s, loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200]TrainLoss: 0.5686 |ValLoss: 1.2133 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/200 - Train: 100%|█| 1/1 [00:00<00:00, 107.19it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/200]TrainLoss: 0.5612 |ValLoss: 1.2395 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/200 - Train: 100%|█| 1/1 [00:00<00:00, 99.94it/s, loss=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/200]TrainLoss: 0.5456 |ValLoss: 1.2655 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/200 - Train: 100%|█| 1/1 [00:00<00:00, 111.08it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/200]TrainLoss: 0.5341 |ValLoss: 1.2916 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/200 - Train: 100%|█| 1/1 [00:00<00:00, 100.01it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/200]TrainLoss: 0.5227 |ValLoss: 1.3179 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/200 - Train: 100%|█| 1/1 [00:00<00:00, 125.03it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/200]TrainLoss: 0.5063 |ValLoss: 1.3444 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/200 - Train: 100%|█| 1/1 [00:00<00:00, 125.02it/s, loss=0."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/200]TrainLoss: 0.4959 |ValLoss: 1.3706 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/200 - Train: 100%|█| 1/1 [00:00<00:00, 142.82it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/200]TrainLoss: 0.4837 |ValLoss: 1.3968 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/200 - Train: 100%|█| 1/1 [00:00<00:00, 134.97it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/200]TrainLoss: 0.4776 |ValLoss: 1.4231 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/200 - Train: 100%|█| 1/1 [00:00<00:00, 111.11it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/200]TrainLoss: 0.4716 |ValLoss: 1.4490 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/200 - Train: 100%|█| 1/1 [00:00<00:00, 112.81it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200]TrainLoss: 0.4604 |ValLoss: 1.4743 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/200 - Train: 100%|█| 1/1 [00:00<00:00, 142.81it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/200]TrainLoss: 0.4527 |ValLoss: 1.4989 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/200 - Train: 100%|█| 1/1 [00:00<00:00, 118.35it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/200]TrainLoss: 0.4478 |ValLoss: 1.5233 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/200 - Train: 100%|█| 1/1 [00:00<00:00, 118.73it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/200]TrainLoss: 0.4355 |ValLoss: 1.5473 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/200 - Train: 100%|█| 1/1 [00:00<00:00, 90.88it/s, loss=0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/200]TrainLoss: 0.4382 |ValLoss: 1.5709 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/200 - Train: 100%|█| 1/1 [00:00<00:00, 128.38it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/200]TrainLoss: 0.4298 |ValLoss: 1.5938 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/200 - Train: 100%|█| 1/1 [00:00<00:00, 124.96it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/200]TrainLoss: 0.4144 |ValLoss: 1.6163 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/200 - Train: 100%|█| 1/1 [00:00<00:00, 139.10it/s, loss=0."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/200]TrainLoss: 0.4138 |ValLoss: 1.6378 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/200 - Train: 100%|█| 1/1 [00:00<00:00, 142.82it/s, loss=0."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/200]TrainLoss: 0.4055 |ValLoss: 1.6590 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/200 - Train: 100%|█| 1/1 [00:00<00:00, 146.40it/s, loss=0."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/200]TrainLoss: 0.4047 |ValLoss: 1.6798 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/200 - Train: 100%|█| 1/1 [00:00<00:00, 194.28it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/200]TrainLoss: 0.4020 |ValLoss: 1.6997 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/200 - Train: 100%|█| 1/1 [00:00<00:00, 142.86it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/200]TrainLoss: 0.3912 |ValLoss: 1.7188 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/200 - Train: 100%|█| 1/1 [00:00<00:00, 142.86it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/200]TrainLoss: 0.3816 |ValLoss: 1.7371 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/200 - Train: 100%|█| 1/1 [00:00<00:00, 249.99it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/200]TrainLoss: 0.3778 |ValLoss: 1.7549 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/200 - Train: 100%|█████| 1/1 [00:00<?, ?it/s, loss=0.3681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/200]TrainLoss: 0.3681 |ValLoss: 1.7727 |ValAcc: 0.0000|Time: 0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/200 - Train: 100%|█████| 1/1 [00:00<?, ?it/s, loss=0.3764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/200]TrainLoss: 0.3764 |ValLoss: 1.7903 |ValAcc: 0.0000|Time: 0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/200 - Train: 100%|█████| 1/1 [00:00<?, ?it/s, loss=0.3653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/200]TrainLoss: 0.3653 |ValLoss: 1.8070 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/200 - Train: 100%|█| 1/1 [00:00<00:00, 62.26it/s, loss=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/200]TrainLoss: 0.3566 |ValLoss: 1.8234 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/200 - Train: 100%|█| 1/1 [00:00<00:00, 65.19it/s, loss=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/200]TrainLoss: 0.3533 |ValLoss: 1.8391 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/200 - Train: 100%|█| 1/1 [00:00<00:00, 62.19it/s, loss=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/200]TrainLoss: 0.3570 |ValLoss: 1.8537 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/200 - Train: 100%|█| 1/1 [00:00<00:00, 62.78it/s, loss=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/200]TrainLoss: 0.3514 |ValLoss: 1.8674 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/200 - Train: 100%|█| 1/1 [00:00<00:00, 62.51it/s, loss=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/200]TrainLoss: 0.3446 |ValLoss: 1.8808 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/200 - Train: 100%|█████| 1/1 [00:00<?, ?it/s, loss=0.3301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/200]TrainLoss: 0.3301 |ValLoss: 1.8940 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/200 - Train: 100%|█████| 1/1 [00:00<?, ?it/s, loss=0.3363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/200]TrainLoss: 0.3363 |ValLoss: 1.9069 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/200 - Train: 100%|█████| 1/1 [00:00<?, ?it/s, loss=0.3244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/200]TrainLoss: 0.3244 |ValLoss: 1.9195 |ValAcc: 0.0000|Time: 0.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/200 - Train: 100%|█████| 1/1 [00:00<?, ?it/s, loss=0.3264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/200]TrainLoss: 0.3264 |ValLoss: 1.9314 |ValAcc: 0.0000|Time: 0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/200 - Train: 100%|█████| 1/1 [00:00<?, ?it/s, loss=0.3202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/200]TrainLoss: 0.3202 |ValLoss: 1.9428 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/200 - Train: 100%|█| 1/1 [00:00<00:00, 326.30it/s, loss=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/200]TrainLoss: 0.3075 |ValLoss: 1.9542 |ValAcc: 0.0000|Time: 0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/200 - Train: 100%|█| 1/1 [00:00<00:00, 62.16it/s, loss=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/200]TrainLoss: 0.3167 |ValLoss: 1.9651 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/200 - Train: 100%|█| 1/1 [00:00<00:00, 63.59it/s, loss=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/200]TrainLoss: 0.3058 |ValLoss: 1.9756 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/200 - Train: 100%|█| 1/1 [00:00<00:00, 62.05it/s, loss=0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/200]TrainLoss: 0.3007 |ValLoss: 1.9857 |ValAcc: 0.0000|Time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/200 - Train: 100%|█| 1/1 [00:00<00:00, 86.74it/s, loss=0.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/200]TrainLoss: 0.3056 |ValLoss: 1.9954 |ValAcc: 0.0000|Time: 0.02s\n",
      "Early stopping triggered.\n",
      "Selesai training fine-tuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CELL 9: INISIALISASI dan EKSEKUSI MODEL dan TRAINING\n",
    "\n",
    "# (dari cell 4 : Build Frozen Parts)\n",
    "variant = 'A' # BAGIAN INI BISA DIGANTI A, B, atau C\n",
    "encoder_frozen, transformer_frozen, proj_head_frozen = build_frozen_parts_from_best_pretrained_model(variant, device)\n",
    "print(\"Komponen pra-trained telah dimuat dan dibekukan.\")\n",
    "\n",
    "# (dari cell 5 : Classifier)\n",
    "# Buat instance model FT\n",
    "model = FTClassifier(encoder_frozen, transformer_frozen, proj_head_frozen, num_classes=2).to(device)\n",
    "# Pastikan hanya parameter classifier yang requires_grad=True\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(\"Jumlah parameter yang dilatih (harus hanya classifier):\", sum(p.numel() for p in trainable_params))\n",
    "\n",
    "# (dari cell 7 : Training Loop)\n",
    "# Jalankan training\n",
    "train_finetune(model, variant, train_loader, val_loader, device, num_epochs=200, lr=1e-3, weight_decay=1e-4, patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa478ef-7fe5-41fd-bf59-777ec4a5ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CPU (temporary debug purpose)\n",
    "\n",
    "# ================== kutipan sedikit keperluan dari cell 9\n",
    "\n",
    "# (dari cell 4 : Build Frozen Parts)\n",
    "variant = 'A' # BAGIAN INI BISA DIGANTI A, B, atau C\n",
    "encoder_frozen, transformer_frozen, proj_head_frozen = build_frozen_parts_from_best_pretrained_model(variant, device)\n",
    "print(\"Komponen pra-trained telah dimuat dan dibekukan.\")\n",
    "\n",
    "# (dari cell 5 : Classifier)\n",
    "# Buat instance model FT\n",
    "model = FTClassifier(encoder_frozen, transformer_frozen, proj_head_frozen, num_classes=2).to(device)\n",
    "# Pastikan hanya parameter classifier yang requires_grad=True\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(\"Jumlah parameter yang dilatih (harus hanya classifier):\", sum(p.numel() for p in trainable_params))\n",
    "\n",
    "# ==================\n",
    "\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"Coba single-batch CPU forward test...\")\n",
    "model_cpu = model.to('cpu')\n",
    "with torch.no_grad():\n",
    "    out = model_cpu(xb)   # pastikan xb di CPU\n",
    "print(\"Sukses CPU forward, out.shape:\", out.shape)\n",
    "# pindahkan model kembali ke device (GPU) bila perlu\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b047966-990a-4516-8225-4f0fd6565ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best fine-tuned classifier from best_finetuned_ver3A.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_14020\\850362123.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  svb = torch.load(saved_best_finetuned_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# CELL 10: MUAT bobot classifier terbaik (jika ada)\n",
    "\n",
    "saved_best_finetuned_path = f\"best_finetuned_ver3{variant}.pt\"\n",
    "\n",
    "if os.path.exists(saved_best_finetuned_path):\n",
    "    svb = torch.load(saved_best_finetuned_path, map_location=device)\n",
    "    model.classifier.load_state_dict(svb[\"classifier_state\"])\n",
    "    print(\"Loaded best fine-tuned classifier from\", saved_best_finetuned_path)\n",
    "else:\n",
    "    print(\"Tidak ditemukan fine-tuned checkpoint. Pastikan training selesai dan file tersimpan.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1222940-ce7a-4e8a-b120-3feba95eaa49",
   "metadata": {},
   "source": [
    "============================== \n",
    "just separator\n",
    "=============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5643575a-4014-44d8-8fcd-32f593c91b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 11: INFERENCE PETA KLASIFIKASI (SLIDING WINDOW PATCH-CENTER) - versi batch untuk efisiensi\n",
    "\n",
    "def inference_map_patch_center(full_image, model, device, patch_size=9, batch_size=256, pad_mode='zero'):\n",
    "    \"\"\"\n",
    "    full_image: numpy (H, W, B)\n",
    "    model: model FTClassifier yang sudah dimuat bobot classifier terbaik\n",
    "    patch_size: 9 (patch spatial)\n",
    "    pad_mode: hanya info; yang akan digunakan adalah zero padding (zeroPadding_3D)\n",
    "    return: pred_map (H, W) int {0,1}\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    H, W, B = full_image.shape\n",
    "    assert B == 224, \"Diharapkan 224 band; sesuaikan bila berbeda.\"\n",
    "\n",
    "    half = patch_size // 2\n",
    "    # Gunakan zero padding (seperti pada preprocessing)\n",
    "    padded = zeroPadding_3D(full_image, half)  # hasil shape (H+2*half, W+2*half, B)\n",
    "\n",
    "    # Pre-buat semua patch dalam bentuk tumpukan (agar memudahkan batching)\n",
    "    coords = []\n",
    "    patches = []\n",
    "    # iterasi per piksel pusat\n",
    "    for i in range(half, half + H):\n",
    "        for j in range(half, half + W):\n",
    "            patch = padded[i-half:i+half+1, j-half:j+half+1, :]  # (9,9,224)\n",
    "            patches.append(patch)\n",
    "            coords.append((i-half, j-half))\n",
    "\n",
    "    patches = np.stack(patches, axis=0)  # shape (H*W, 9,9,224)\n",
    "    N = patches.shape[0]\n",
    "\n",
    "    # Konversi ke tensor conv3d format (N,1,224,9,9)\n",
    "    X = torch.tensor(patches, dtype=torch.float32).unsqueeze(1).permute(0,1,4,2,3)\n",
    "\n",
    "    loader = DataLoader(TensorDataset(X, torch.zeros(len(X), dtype=torch.long)),\n",
    "                        batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in tqdm(loader, desc=\"Inferensi peta (batch)\"):\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)            # (B,2)\n",
    "            p = logits.argmax(dim=1).cpu().numpy()\n",
    "            preds.append(p)\n",
    "    preds = np.concatenate(preds, axis=0)  # (H*W, )\n",
    "\n",
    "    pred_map = preds.reshape(H, W)\n",
    "    return pred_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c9b3c88-aa5f-4f5e-a7a6-f077e59a21ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GM01 shapes -> img: (1243, 684, 224) | gt: (1243, 684)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 61704986112 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGM01 shapes -> img:\u001b[39m\u001b[38;5;124m\"\u001b[39m, img\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| gt:\u001b[39m\u001b[38;5;124m\"\u001b[39m, gt_map\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Jalankan inference (Peringatan : Proses ini mungkin memakan memori & waktu)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m pred_map_gm01 \u001b[38;5;241m=\u001b[39m \u001b[43minference_map_patch_center\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Simpan peta prediksi\u001b[39;00m\n\u001b[0;32m     17\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/result/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[11], line 33\u001b[0m, in \u001b[0;36minference_map_patch_center\u001b[1;34m(full_image, model, device, patch_size, batch_size, pad_mode)\u001b[0m\n\u001b[0;32m     30\u001b[0m N \u001b[38;5;241m=\u001b[39m patches\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Konversi ke tensor conv3d format (N,1,224,9,9)\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     35\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(TensorDataset(X, torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(X), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)),\n\u001b[0;32m     36\u001b[0m                     batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     38\u001b[0m preds \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 61704986112 bytes."
     ]
    }
   ],
   "source": [
    "# CELL 12: MUAT GM01.mat -> JALANKAN INFERENCE -> SIMPAN PETA\n",
    "\n",
    "# mat_path = \"D:/CurrentlyActiveResearch/oilspill_project/data/raw/GM01.mat\" # absolute path\n",
    "mat_path = \"../data/raw/GM01.mat\" # relative path\n",
    "assert os.path.exists(mat_path), f\"File GM01.mat tidak ditemukan di {mat_path}\"\n",
    "\n",
    "mat = sio.loadmat(mat_path)\n",
    "img = mat[\"img\"]    # (H, W, B)\n",
    "gt_map = mat[\"map\"] # (H, W)\n",
    "\n",
    "print(\"GM01 shapes -> img:\", img.shape, \"| gt:\", gt_map.shape)\n",
    "\n",
    "# Jalankan inference (Peringatan : Proses ini mungkin memakan memori & waktu)\n",
    "pred_map_gm01 = inference_map_patch_center(img, model, device, patch_size=9, batch_size=512)\n",
    "\n",
    "# Simpan peta prediksi\n",
    "save_dir = \"../data/result/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "np.save(os.path.join(save_dir, \"pred_map_GM01.npy\"), pred_map_gm01)\n",
    "\n",
    "print(\"Peta prediksi GM01 disimpan ke pred_map_GM01.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1314df-f997-47ef-91af-4b481c74db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13: EVALUASI PETA\n",
    "\n",
    "saved_pred_map_path = os.path.join(save_dir, \"pred_map_GM01.npy\")\n",
    "assert os.path.exists(saved_pred_map_path), f\"File pred_map_GM01.npy tidak ditemukan\"\n",
    "\n",
    "pred_map = np.load(saved_pred_map_path)\n",
    "gt = gt_map.astype(int)\n",
    "\n",
    "# Flatten untuk metrik\n",
    "y_pred = pred_map.flatten()\n",
    "y_true = gt.flatten()\n",
    "\n",
    "oa = accuracy_score(y_true, y_pred)\n",
    "f1_per_class = f1_score(y_true, y_pred, average=None)  # per class\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Overall Accuracy (OA):\", oa)\n",
    "print(\"F1 per kelas:\", f1_per_class)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "print(\"\\nReport klasifikasi (per-class precision/recall/f1):\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56349420-e879-4cad-9c8b-9a665cce9adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 14: VISUALISASI PETA PREDIKSI dan GT\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Citra (band visualisasi contoh)\")\n",
    "# untuk visual: ambil 3 pita (mis. 30, 20, 10)\n",
    "b1, b2, b3 = 30, 20, 10\n",
    "rgb = img[:,:, [b1,b2,b3]]\n",
    "# normalisasi untuk tampil\n",
    "rgb_norm = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n",
    "plt.imshow(rgb_norm)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Ground Truth GM01\")\n",
    "plt.imshow(gt, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Prediksi GM01\")\n",
    "plt.imshow(pred_map_gm01, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
